\section{Discussion}

Since our illustrative examples are more of sanity checks than benchmark experiments, we compare the methods in terms of their implementation and computational cost, rather than their performance. While Lapalce approximation is applicable in a post-hoc manner, constructing the approximation is non-trivial due to challenges in Hessian computation. Furthermore, the choice of prior precision as a hyperparameter turns out to be crucial for the quality of uncertainty representation. By constrast, Deep Ensembles and MC Dropout require less implementation effort and less fine-tuning. However, they are less efficient in comparison to some variants of Laplace approximation, since it is necessary to perform multiple forward passes to obtain a single uncertainty estimate. While Deep Ensemble captures multimodality, its computational cost is much higher than the other two methods. MC Dropout seems a good alternative to Laplace approximation and Deep Ensembles if one pursues both low implementation effort and low computational cost. However, its use cases are limited to networks trained with Dropout.

\section{Conclusion and outlook}

In light of the equivalence between regulized risk minimization and MAP estimation, we presented a Bayesian view of deep learning. Our discussion was centered around the predictive distribution which accounts for both aleatroic and espistemic uncertainty as two major types of uncertainty in neural networks. We introduced three methods for approximating this predictive distribution and discussed their strengths and weaknesses.

Throughout this paper, we did not mention how to quantitatively assess these approximation methods. Benchmarking uncertainty quantification is, in fact, a difficult task, because there is no ground-truth uncertainty as reference \citep{lakshminarayananSimpleScalablePredictive2017b}. One might be able to control the aleatoric uncertainty by designing a toy experiment, but the epistemic uncertainty inside the model remains unknown. This observation also raises the question whether we can separate these two types of uncertainty. Current research shows that uncertainty disentaglement is hard \citep{mucsanyiBenchmarkingUncertaintyDisentanglement2024a,wimmerQuantifyingAleatoricEpistemic2023}. Despite the open challenges, uncertainty quantification methods have been successfully applied to active learning, medical image analysis, robotics, and earth observation \citep{gawlikowskiSurveyUQ2023}.
