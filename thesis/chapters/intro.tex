\section{Introduction}
\label{sec:intro}

Deep neural netwrks (DNNs) have demonstrated astonishing capabilites of tackling complex predictive tasks in computer vision, natural language processing, and various other fields of science and engineering. While they are often able to achieve high predictive accuracy given large volumes of training data, DNNs are often untrustworthy in real-world applications.

Recent research has highlighted that noisy, corrupted input data can easily mislead neural networks to make wrong predictions. For instance, covering a part of the input image with noise causes a DNN-based vehicle control system to fail at steering angle prediction, which could put human lives in danger \citep{loquercioAutoDrive2020}. Even a barely perceptible noise, if designed in a certain way, can completely irritate DNN image classifiers \citep{moosaviAdversarial2017}. In the other extreme, certain noise patterns unrecognizable to humans get classified by neural networks as wild animals or everyday objects, and the predictive probabilities assigned to these false classes are very high \citep{nguyenFoolingDNN2015}. Predictions with unreasonably high confidence are not limited to the case where the input data are fine-tuned noise. \cite{guoCalibration2017} showed that over-confidence is a common phenomenon in DNNs. Apart from the objective shortcomings, how human users perceive neural networks also plays a role. In the clinical context, for example, people tend not to trust neural networks due to their nature of being black-box models \citep{royBrainSegment2019}.

One approach to making neural networks more trustworthy is to equip them with uncertainty estimation. Besides the predicted label, the model should report the degree of uncertainty in its prediction. If the potentialy unreliable model is uncertain, a reliable human expert can be called to intervene.

In \cref{sec:bayesian-dl}, we present the Bayesian framework for uncertainty quantification in neural networks. After setting up this theoretical foundation, we introduce three popular uncertainty quantification methods. In \cref{sec:la}, we derive Laplace approximation and present some of its extensions. In \cref{sec:de}, we motivate the use of ensemble methods for uncertainty quantification from a Bayesian perspective. In \cref{sec:mcd}, we introduce Monte Carlo Dropout, which can be motivated as an ensemble method but enjoys a Bayesian justification. Finally, we apply these methods on toy experiments in \cref{sec:exp}.
