\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{loquercioAutoDrive2020}
\citation{moosaviAdversarial2017}
\citation{nguyenFoolingDNN2015}
\citation{guoCalibration2017}
\citation{royBrainSegment2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian Deep Learning}{2}{section.2}\protected@file@percent }
\newlabel{sec:bayesian-dl}{{2}{2}{Bayesian Deep Learning}{section.2}{}}
\newlabel{sec:bayesian-dl@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks as Probabilistic Models}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:nn}{{2.1}{2}{Neural Networks as Probabilistic Models}{subsection.2.1}{}}
\newlabel{sec:nn@cref}{{[subsection][1][2]2.1}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Regularization and MAP Estimation}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec:regularization}{{2.2}{3}{Regularization and MAP Estimation}{subsection.2.2}{}}
\newlabel{sec:regularization@cref}{{[subsection][2][2]2.2}{[1][3][]3}{}{}{}}
\citation{wilsonBayesianDeepLearning2020}
\citation{wilsonBayesianDeepLearning2020,gawlikowskiSurveyUQ2023}
\newlabel{eq:l2-reg}{{1}{4}{Regularization and MAP Estimation}{equation.1}{}}
\newlabel{eq:l2-reg@cref}{{[equation][1][]1}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Predictive Distribution}{4}{subsection.2.3}\protected@file@percent }
\newlabel{sec:bma}{{2.3}{4}{Predictive Distribution}{subsection.2.3}{}}
\newlabel{sec:bma@cref}{{[subsection][3][2]2.3}{[1][4][]4}{}{}{}}
\newlabel{eq:predictive}{{2}{4}{Predictive Distribution}{equation.2}{}}
\newlabel{eq:predictive@cref}{{[equation][2][]2}{[1][4][]4}{}{}{}}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{gawlikowskiSurveyUQ2023}
\citation{malininPredictiveUncertaintyEstimation2018}
\citation{hullermeierAleatoricEpistemicUncertainty2021}
\citation{gawlikowskiSurveyUQ2023}
\citation{hullermeierAleatoricEpistemicUncertainty2021}
\citation{galUncertaintyTypes2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Aleatoric and Epistemic Uncertainty}{5}{subsection.2.4}\protected@file@percent }
\newlabel{sec:uncertainty}{{2.4}{5}{Aleatoric and Epistemic Uncertainty}{subsection.2.4}{}}
\newlabel{sec:uncertainty@cref}{{[subsection][4][2]2.4}{[1][5][]5}{}{}{}}
\citation{hullermeierAleatoricEpistemicUncertainty2021}
\citation{galUncertaintyTypes2016}
\citation{hullermeierAleatoricEpistemicUncertainty2021}
\citation{daxbergerLaplaceRedux2021}
\@writefile{toc}{\contentsline {section}{\numberline {3}Laplace Approximation}{7}{section.3}\protected@file@percent }
\newlabel{sec:la}{{3}{7}{Laplace Approximation}{section.3}{}}
\newlabel{sec:la@cref}{{[section][3][]3}{[1][7][]7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Standard Laplace Approximation}{7}{subsection.3.1}\protected@file@percent }
\newlabel{eq:hessian}{{3}{7}{Standard Laplace Approximation}{equation.3}{}}
\newlabel{eq:hessian@cref}{{[equation][3][]3}{[1][7][]7}{}{}{}}
\newlabel{eq:la}{{4}{7}{Standard Laplace Approximation}{equation.4}{}}
\newlabel{eq:la@cref}{{[equation][4][]4}{[1][7][]7}{}{}{}}
\citation{ritterScalableLA2018,kristiadiABitBayesian2020,immerLLA2021a}
\citation{ritterScalableLA2018}
\citation{immerMargLik2021}
\citation{daxbergerLaplaceRedux2021}
\citation{immerLLA2021a}
\citation{spiegelhalter1990ProbitApprox1,mackay1992ProbitApprox2}
\citation{gibbs1997ExtendProbit}
\citation{hobbhahnyLapaceBridge2022a}
\newlabel{eq:hessian-gaussian-prior}{{5}{8}{Standard Laplace Approximation}{equation.5}{}}
\newlabel{eq:hessian-gaussian-prior@cref}{{[equation][5][]5}{[1][7][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Linearized Laplace Approximation}{8}{subsection.3.2}\protected@file@percent }
\newlabel{eq:linearize}{{6}{8}{Linearized Laplace Approximation}{equation.6}{}}
\newlabel{eq:linearize@cref}{{[equation][6][]6}{[1][8][]8}{}{}{}}
\newlabel{eq:lla}{{7}{8}{Linearized Laplace Approximation}{equation.7}{}}
\newlabel{eq:lla@cref}{{[equation][7][]7}{[1][8][]8}{}{}{}}
\newlabel{eq:predictive-lla-classif}{{8}{8}{Linearized Laplace Approximation}{equation.8}{}}
\newlabel{eq:predictive-lla-classif@cref}{{[equation][8][]8}{[1][8][]8}{}{}{}}
\citation{immerLLA2021a}
\citation{kristiadiABitBayesian2020}
\citation{daxbergerSubnetLA2021}
\citation{daxbergerLaplaceRedux2021}
\citation{kristiadiABitBayesian2020}
\citation{eschenhagenMixturesLaplace2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Last-layer Laplace Approximation}{9}{subsection.3.3}\protected@file@percent }
\citation{liVisualizingLossLandscape2018}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{livierisEnsembleTechniquesWeightconstrained2021}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{wilsonBayesianDeepLearning2020}
\citation{sommerConnectingDotsModeConnectedness2024b}
\citation{grigsbyHiddenSymmetriesReLU2023}
\@writefile{toc}{\contentsline {section}{\numberline {4}Ensembles Methods}{10}{section.4}\protected@file@percent }
\newlabel{sec:de}{{4}{10}{Ensembles Methods}{section.4}{}}
\newlabel{sec:de@cref}{{[section][4][]4}{[1][10][]10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Deep Ensembles}{10}{subsection.4.1}\protected@file@percent }
\newlabel{eq:ensemble}{{9}{10}{Deep Ensembles}{equation.9}{}}
\newlabel{eq:ensemble@cref}{{[equation][9][]9}{[1][10][]10}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Symmetry of Modes}{10}{subsection.4.2}\protected@file@percent }
\citation{fortDeepEnsemblesLoss2020}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{valdenegro-toroDeepSubEnsemblesFast2019}
\citation{valdenegro-toroDeepSubEnsemblesFast2019}
\citation{wenBatchEnsembleAlternativeApproach2019}
\citation{gawlikowskiSurveyUQ2023}
\citation{wenBatchEnsembleAlternativeApproach2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Less Expensive Alternatives}{11}{subsection.4.3}\protected@file@percent }
\citation{hintonDropout2012}
\citation{hintonDropout2012}
\citation{srivastavaDropout2014}
\citation{srivastavaDropout2014}
\citation{hintonDropout2012}
\@writefile{toc}{\contentsline {section}{\numberline {5}Monte Carlo Dropout}{12}{section.5}\protected@file@percent }
\newlabel{sec:mcd}{{5}{12}{Monte Carlo Dropout}{section.5}{}}
\newlabel{sec:mcd@cref}{{[section][5][]5}{[1][12][]12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dropout Mechanism}{12}{subsection.5.1}\protected@file@percent }
\newlabel{sec:dropout-mechanism}{{5.1}{12}{Dropout Mechanism}{subsection.5.1}{}}
\newlabel{sec:dropout-mechanism@cref}{{[subsection][1][5]5.1}{[1][12][]12}{}{}{}}
\citation{galDropoutBayesianApproximation2016}
\citation{srivastavaDropout2014}
\citation{galDropoutBayesianApproximation2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Dropout for Uncertainty Quantification}{13}{subsection.5.2}\protected@file@percent }
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Predictions of a standard MLP. The blue and red points are training data. The color in the background represents the predicted probability $p(y \,|\,\bm  {x}, \hat  {\bm  {\theta }}_\text  {MAP})$ at each point $\bm  {x} = (x_1, x_2)^\top $ in the feature space.}}{14}{figure.1}\protected@file@percent }
\newlabel{fig:classif-map}{{1}{14}{Predictions of a standard MLP. The blue and red points are training data. The color in the background represents the predicted probability $p(y \given \vec {x}, \hat {\vec {\theta }}_\text {MAP})$ at each point $\vec {x} = (x_1, x_2)^\T $ in the feature space}{figure.1}{}}
\newlabel{fig:classif-map@cref}{{[figure][1][]1}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Illustrative Examples}{14}{section.6}\protected@file@percent }
\newlabel{sec:exp}{{6}{14}{Illustrative Examples}{section.6}{}}
\newlabel{sec:exp@cref}{{[section][6][]6}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Binary Classification}{14}{subsection.6.1}\protected@file@percent }
\newlabel{sec:classif}{{6.1}{14}{Binary Classification}{subsection.6.1}{}}
\newlabel{sec:classif@cref}{{[subsection][1][6]6.1}{[1][14][]14}{}{}{}}
\citation{kristiadiLearnableUncertaintyLaplace2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Laplace approximation for different prior precisions $\tau ^{-2}$. Monte Carlo integration is used to approximate the predictive distribution. For each test point $(x_1, x_2)^\top $, 1000 parameters are sampled from the Gaussian approximate posterior.}}{15}{figure.2}\protected@file@percent }
\newlabel{fig:classif-la}{{2}{15}{Laplace approximation for different prior precisions $\tau ^{-2}$. Monte Carlo integration is used to approximate the predictive distribution. For each test point $(x_1, x_2)^\T $, 1000 parameters are sampled from the Gaussian approximate posterior}{figure.2}{}}
\newlabel{fig:classif-la@cref}{{[figure][2][]2}{[1][15][]15}{}{}{}}
\citation{daxbergerLaplaceRedux2021}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Deep ensemble of 10 models. The first five plots show five individual models. The last plot shows the ensemble average.}}{16}{figure.3}\protected@file@percent }
\newlabel{fig:classif-ensemble}{{3}{16}{Deep ensemble of 10 models. The first five plots show five individual models. The last plot shows the ensemble average}{figure.3}{}}
\newlabel{fig:classif-ensemble@cref}{{[figure][3][]3}{[1][15][]16}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Regression}{16}{subsection.6.2}\protected@file@percent }
\citation{lakshminarayananSimpleScalablePredictive2017b}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces MC Dropout. For each test point $(x_1, x_2)^\top $, a sample of 1000 parameters is used (same as for Laplace approximation).}}{17}{figure.4}\protected@file@percent }
\newlabel{fig:classif-mcd}{{4}{17}{MC Dropout. For each test point $(x_1, x_2)^\T $, a sample of 1000 parameters is used (same as for Laplace approximation)}{figure.4}{}}
\newlabel{fig:classif-mcd@cref}{{[figure][4][]4}{[1][16][]17}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of MAP and uncertainty quantification methods on the regression toy dataset. The blue points as training data are sampled with Gaussian noise from the grey line. For the MAP plot, the red curve represents the prediction function. For the other plots, the predictive mean (red curve) $\pm $ two standard deviations (orange band) are shown.}}{18}{figure.5}\protected@file@percent }
\newlabel{fig:regr}{{5}{18}{Comparison of MAP and uncertainty quantification methods on the regression toy dataset. The blue points as training data are sampled with Gaussian noise from the grey line. For the MAP plot, the red curve represents the prediction function. For the other plots, the predictive mean (red curve) $\pm $ two standard deviations (orange band) are shown}{figure.5}{}}
\newlabel{fig:regr@cref}{{[figure][5][]5}{[1][17][]18}{}{}{}}
\citation{lakshminarayananSimpleScalablePredictive2017b}
\citation{mucsanyiBenchmarkingUncertaintyDisentanglement2024a,wimmerQuantifyingAleatoricEpistemic2023}
\citation{gawlikowskiSurveyUQ2023}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{19}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion and outlook}{19}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Electronic appendix}{V}{appendix.A}\protected@file@percent }
\newlabel{el_app}{{A}{V}{Electronic appendix}{appendix.A}{}}
\newlabel{el_app@cref}{{[section][1][]A}{[I][5][]V}{}{}{}}
\bibdata{bibliography}
\bibcite{daxbergerLaplaceRedux2021}{{1}{2021}{{Daxberger, Kristiadi, Immer, Eschenhagen, Bauer and\ Hennig}}{{}}}
\bibcite{daxbergerSubnetLA2021}{{2}{2021}{{Daxberger, Nalisnick, Allingham, Antoran and\ {Hernandez-Lobato}}}{{}}}
\bibcite{eschenhagenMixturesLaplace2021}{{3}{2021}{{Eschenhagen et~al.}}{{Eschenhagen, Daxberger, Hennig and\ Kristiadi}}}
\bibcite{fortDeepEnsemblesLoss2020}{{4}{2020}{{Fort et~al.}}{{Fort, Hu and\ Lakshminarayanan}}}
\bibcite{galUncertaintyTypes2016}{{5}{2016}{{Gal}}{{}}}
\bibcite{galDropoutBayesianApproximation2016}{{6}{2016}{{Gal and\ Ghahramani}}{{}}}
\bibcite{gawlikowskiSurveyUQ2023}{{7}{2023}{{Gawlikowski et~al.}}{{Gawlikowski, Tassi, Ali, Lee, Humt, Feng, Kruspe, Triebel, Jung, Roscher, Shahzad, Yang, Bamler and\ Zhu}}}
\bibcite{gibbs1997ExtendProbit}{{8}{1997}{{Gibbs}}{{}}}
\bibcite{grigsbyHiddenSymmetriesReLU2023}{{9}{2023}{{Grigsby et~al.}}{{Grigsby, Lindsey and\ Rolnick}}}
\bibcite{guoCalibration2017}{{10}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun and\ Weinberger}}}
\bibcite{hintonDropout2012}{{11}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever and\ Salakhutdinov}}}
\bibcite{hobbhahnyLapaceBridge2022a}{{12}{2022}{{Hobbhahn et~al.}}{{Hobbhahn, Kristiadi and\ Hennig}}}
\bibcite{hullermeierAleatoricEpistemicUncertainty2021}{{13}{2021}{{H{\"u}llermeier and\ Waegeman}}{{}}}
\bibcite{immerMargLik2021}{{14}{2021}{{Immer, Bauer, Fortuin, R{\"a}tsch and\ Emtiyaz}}{{}}}
\bibcite{immerLLA2021a}{{15}{2021}{{Immer, Korzepa and\ Bauer}}{{}}}
\bibcite{kristiadiABitBayesian2020}{{16}{2020}{{Kristiadi et~al.}}{{Kristiadi, Hein and\ Hennig}}}
\bibcite{kristiadiLearnableUncertaintyLaplace2021}{{17}{2021}{{Kristiadi et~al.}}{{Kristiadi, Hein and\ Hennig}}}
\bibcite{lakshminarayananSimpleScalablePredictive2017b}{{18}{2017}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel and\ Blundell}}}
\bibcite{liVisualizingLossLandscape2018}{{19}{2018}{{Li et~al.}}{{Li, Xu, Taylor, Studer and\ Goldstein}}}
\bibcite{livierisEnsembleTechniquesWeightconstrained2021}{{20}{2021}{{Livieris et~al.}}{{Livieris, Iliadis and\ Pintelas}}}
\bibcite{loquercioAutoDrive2020}{{21}{2020}{{Loquercio et~al.}}{{Loquercio, Segu and\ Scaramuzza}}}
\bibcite{mackay1992ProbitApprox2}{{22}{1992}{{MacKay}}{{}}}
\bibcite{malininPredictiveUncertaintyEstimation2018}{{23}{2018}{{Malinin and\ Gales}}{{}}}
\bibcite{moosaviAdversarial2017}{{24}{2017}{{Moosavi-Dezfooli et~al.}}{{Moosavi-Dezfooli, Fawzi, Fawzi and\ Frossard}}}
\bibcite{mucsanyiBenchmarkingUncertaintyDisentanglement2024a}{{25}{2024}{{Mucs{\'a}nyi et~al.}}{{Mucs{\'a}nyi, Kirchhof and\ Oh}}}
\bibcite{nguyenFoolingDNN2015}{{26}{2015}{{Nguyen et~al.}}{{Nguyen, Yosinski and\ Clune}}}
\bibcite{scikit-learn}{{27}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot and\ Duchesnay}}}
\bibcite{ritterScalableLA2018}{{28}{2018}{{Ritter et~al.}}{{Ritter, Botev and\ Barber}}}
\bibcite{royBrainSegment2019}{{29}{2019}{{Roy et~al.}}{{Roy, Conjeti, Navab and\ Wachinger}}}
\bibcite{sommerConnectingDotsModeConnectedness2024b}{{30}{2024}{{Sommer et~al.}}{{Sommer, Wimmer, Papamarkou, Bothmann, Bischl and\ R{\"u}gamer}}}
\bibcite{spiegelhalter1990ProbitApprox1}{{31}{1990}{{Spiegelhalter and\ Lauritzen}}{{}}}
\bibcite{srivastavaDropout2014}{{32}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever and\ Salakhutdinov}}}
\bibcite{valdenegro-toroDeepSubEnsemblesFast2019}{{33}{2019}{{Valdenegro-Toro}}{{}}}
\bibcite{wenBatchEnsembleAlternativeApproach2019}{{34}{2019}{{Wen et~al.}}{{Wen, Tran and\ Ba}}}
\bibcite{wilsonBayesianDeepLearning2020}{{35}{2020}{{Wilson and\ Izmailov}}{{}}}
\bibcite{wimmerQuantifyingAleatoricEpistemic2023}{{36}{2023}{{Wimmer et~al.}}{{Wimmer, Sale, Hofman, Bischl and\ H{\"u}llermeier}}}
\bibstyle{dcu}
\gdef \@abspage@last{27}
